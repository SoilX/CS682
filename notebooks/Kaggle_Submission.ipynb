{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kaggle Submission",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9OJCo-FHEXU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "368525d9-6cd6-43c0-bdc4-9957466fad0c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sIxlnEGHo6R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "b6ad8a03-0a28-434a-833a-938ee1f5e62e"
      },
      "source": [
        "!tar xvzf drive/'My Drive'/csvs.tar\n",
        "!tar xvzf drive/'My Drive'/txt.tar\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.cluster import SpectralClustering\n",
        "\n",
        "# Plotting Dependencies\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Preprocessing dependencies\n",
        "import csv\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import KFold\n",
        "#!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embedded_data.csv\n",
            "embedded_data_training.csv\n",
            "sample_submission.csv\n",
            "train.csv\n",
            "ID.txt\n",
            "ID_training.txt\n",
            "ID_tuned.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgIDUyiNH79t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import numpy as np \n",
        "x = []\n",
        "with open('embedded_data_training.csv') as csvfile:\n",
        "  reader = csv.reader(csvfile, delimiter = ' ')\n",
        "  for row in reader:\n",
        "    x.append([float(n) for n in row])\n",
        "\n",
        "training = np.asarray(x)\n",
        "\n",
        "x = []\n",
        "with open('embedded_data.csv') as csvfile:\n",
        "  reader = csv.reader(csvfile, delimiter = ' ')\n",
        "  for row in reader:\n",
        "    x.append([float(n) for n in row])\n",
        "test = np.asarray(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQT2NHSRJkEu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " def PCA_iterative(data, threshold, mode = 'threshold', whiten_bool = 'False', n = 0):\n",
        "   \"\"\"\n",
        "   mode = 'components' does PCA reduction to n components\n",
        "   mode = 'threshold' performs PCA reduction to the fewest components that satisfy a threshold of explained variance\n",
        "   prints out how many components the data was reduced to\n",
        "   prints out \n",
        "   \"\"\"\n",
        "   if mode == 'threshold':\n",
        "     for i in range(2, data.shape[1]+1):\n",
        "       pca = PCA(n_components = i, whiten=whiten_bool)\n",
        "       B = pca.fit_transform(data)\n",
        "       L = pca.explained_variance_\n",
        "       cl=np.cumsum(L); \n",
        "       if (cl[i-2]/cl[-1]) > threshold:\n",
        "         break\n",
        "   else:\n",
        "     pca = PCA(n_components = n, whiten=whiten_bool)\n",
        "     B = pca.fit_transform(data) \n",
        "     L = pca.explained_variance_\n",
        "     cl=np.cumsum(L); \n",
        "     print('PCA reduction to ', n, ' components with', cl[i-2]/cl[-1], ' explained variance')\n",
        " \n",
        "   # PCA scree plot\n",
        "   plt.subplot(111); \n",
        "   plt.ylabel('Total Variance');\n",
        "   plt.xlabel('Principal component')\n",
        "   plt.plot(np.arange(1,L.shape[0]+1),cl/cl[-1],'o-r'); \n",
        "   plt.ylim(0,None);\n",
        "   return B, pca"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hy4d_2DJ6VZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae5rdbviLpSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZwA6OmeKSKO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = pd.read_csv(\"train.csv\")\n",
        "train_ids = pd.read_csv(\"ID_training.txt\", header=None, names=['image_id'])\n",
        "test_ids = pd.read_csv(\"ID.txt\", header=None, names=['image_id'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKsryoa9JUBW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "a8bf3637-da80-4aa0-8433-3bd955f360a8"
      },
      "source": [
        "data, pca = PCA_iterative(training, 0.99, mode = 'threshold')\n",
        "gmm = GaussianMixture(n_components=3, n_init=10).fit(data)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEJCAYAAACZjSCSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z76GfnxmH7zb",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOmqwLTnKYKN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "ebdc62ca-8e6b-420b-e893-22006f6fde4c"
      },
      "source": [
        "sample = pca.transform(test)\n",
        "sample_scores = sample @ gmm.means_.T\n",
        "data_scores = data @ gmm.means_.T\n",
        "data_scores"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.69768901, -1.29348154,  2.35216378],\n",
              "       [ 1.13710815, -1.3480418 ,  1.29554647],\n",
              "       [-0.81845363, -0.86048202,  1.78444569],\n",
              "       ...,\n",
              "       [ 1.42867331,  0.03349237, -0.93549213],\n",
              "       [ 1.67619919, -0.98582606,  0.42375979],\n",
              "       [-1.75084657, -1.06581782,  2.66726469]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aa0HeE__KfRy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = []\n",
        "labels = []\n",
        "for i, row in train_labels.iterrows():\n",
        "  batches = [float(x) for x in row[\"PredictionString\"].split()]\n",
        "  for j, _ in enumerate(batches[::7]):\n",
        "    result = np.array(batches[j*7:j*7 + 7])[1:]\n",
        "    train_data.append(data_scores[train_ids.image_id == f\"crop_{row.ImageId}_{j}.jpg\"])\n",
        "    labels.append(result)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMkRQMaQQqsr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "x = torch.from_numpy(np.array(train_data)).float()\n",
        "y = torch.from_numpy(np.array(labels)).float()\n",
        "dataset = TensorDataset(x, y)\n",
        "\n",
        "train_loader = DataLoader(dataset=dataset, batch_size=1024, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJSYjg56NbPl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "outputId": "49c33c8a-8f1c-4bfa-96ce-4fe069897eba"
      },
      "source": [
        "from torch.autograd import Variable\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "print(x, y)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[-1.3333, -0.7872,  1.9949]],\n",
            "\n",
            "        [[-1.2017, -0.6583,  1.7220]],\n",
            "\n",
            "        [[ 0.1360, -0.2334,  0.2621]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.7549, -1.0552,  0.4780]],\n",
            "\n",
            "        [[-1.8307,  1.6346, -1.2908]],\n",
            "\n",
            "        [[-0.1638,  0.7943, -1.0773]]]) tensor([[ 2.5484e-01, -2.5753e+00, -3.1026e+00,  7.9654e+00,  3.2007e+00,\n",
            "          1.1023e+01],\n",
            "        [ 1.8165e-01, -1.4695e+00, -3.1216e+00,  9.6033e+00,  4.6663e+00,\n",
            "          1.9339e+01],\n",
            "        [ 1.6307e-01, -1.5687e+00, -3.1175e+00,  1.0390e+01,  1.1222e+01,\n",
            "          5.9783e+01],\n",
            "        ...,\n",
            "        [ 1.4320e-01,  3.0294e+00, -3.1307e+00, -3.2069e+01,  1.8299e+01,\n",
            "          1.1160e+02],\n",
            "        [ 8.7367e-02, -9.3700e-02, -3.1173e+00,  2.4831e-01,  2.2632e+01,\n",
            "          1.3451e+02],\n",
            "        [ 8.7367e-02,  3.6273e-01, -3.1173e+00,  3.3802e+01,  2.5519e+01,\n",
            "          1.3359e+02]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVPSByPdNy7O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "58d7570b-6cd7-4086-c7ee-251289d0976b"
      },
      "source": [
        "class LinearRegressionModel(torch.nn.Module): \n",
        "  \n",
        "    def __init__(self): \n",
        "        super(LinearRegressionModel, self).__init__() \n",
        "        self.linear = torch.nn.Linear(3, 6)\n",
        "  \n",
        "    def forward(self, x): \n",
        "        y_pred = self.linear(x) \n",
        "        return y_pred \n",
        "\n",
        "model = LinearRegressionModel() \n",
        "  \n",
        "criterion = torch.nn.MSELoss() \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.001) \n",
        "  \n",
        "for epoch in range(10):\n",
        "    for x_batch, y_batch in train_loader:\n",
        "      # Forward pass: Compute predicted y by passing  \n",
        "      # x to the model \n",
        "      pred_y = model(x_batch)\n",
        "    \n",
        "      # Compute and print loss \n",
        "      loss = criterion(pred_y, y_batch) \n",
        "    \n",
        "      # Zero gradients, perform a backward pass,  \n",
        "      # and update the weights. \n",
        "      optimizer.zero_grad() \n",
        "      loss.backward() \n",
        "      optimizer.step() \n",
        "      print('epoch {}, loss {}'.format(epoch, loss)) \n",
        "    "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([1024, 6])) that is different to the input size (torch.Size([1024, 1, 6])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 0, loss 727.2955932617188\n",
            "epoch 0, loss 763.5768432617188\n",
            "epoch 0, loss 676.3789672851562\n",
            "epoch 0, loss 674.0404052734375\n",
            "epoch 0, loss 702.9920654296875\n",
            "epoch 0, loss 713.0496215820312\n",
            "epoch 0, loss 715.693359375\n",
            "epoch 0, loss 622.5711669921875\n",
            "epoch 0, loss 686.9867553710938\n",
            "epoch 0, loss 686.51708984375\n",
            "epoch 0, loss 721.822998046875\n",
            "epoch 0, loss 685.593994140625\n",
            "epoch 0, loss 666.2108154296875\n",
            "epoch 0, loss 663.8146362304688\n",
            "epoch 0, loss 691.1473999023438\n",
            "epoch 0, loss 669.71826171875\n",
            "epoch 0, loss 627.4571533203125\n",
            "epoch 0, loss 707.8248901367188\n",
            "epoch 0, loss 687.300048828125\n",
            "epoch 0, loss 740.486572265625\n",
            "epoch 0, loss 675.1241455078125\n",
            "epoch 0, loss 714.4960327148438\n",
            "epoch 0, loss 755.3826293945312\n",
            "epoch 0, loss 659.1583862304688\n",
            "epoch 0, loss 631.6250610351562\n",
            "epoch 0, loss 666.9493408203125\n",
            "epoch 0, loss 680.7713623046875\n",
            "epoch 0, loss 671.438720703125\n",
            "epoch 0, loss 676.9125366210938\n",
            "epoch 0, loss 665.8068237304688\n",
            "epoch 0, loss 2831.981689453125\n",
            "epoch 0, loss 698.9081420898438\n",
            "epoch 0, loss 686.1232299804688\n",
            "epoch 0, loss 722.8797607421875\n",
            "epoch 0, loss 713.9017944335938\n",
            "epoch 0, loss 633.94775390625\n",
            "epoch 0, loss 678.4130859375\n",
            "epoch 0, loss 2760.491455078125\n",
            "epoch 0, loss 706.7161254882812\n",
            "epoch 0, loss 753.1619262695312\n",
            "epoch 0, loss 656.5625\n",
            "epoch 0, loss 682.0276489257812\n",
            "epoch 0, loss 636.1162109375\n",
            "epoch 0, loss 676.2158813476562\n",
            "epoch 0, loss 607.6668090820312\n",
            "epoch 0, loss 748.1044921875\n",
            "epoch 0, loss 667.1439208984375\n",
            "epoch 0, loss 655.5051879882812\n",
            "epoch 0, loss 695.9924926757812\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([532, 6])) that is different to the input size (torch.Size([532, 1, 6])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 1, loss 691.1453857421875\n",
            "epoch 1, loss 667.169677734375\n",
            "epoch 1, loss 685.234130859375\n",
            "epoch 1, loss 676.946533203125\n",
            "epoch 1, loss 675.5419921875\n",
            "epoch 1, loss 641.4351806640625\n",
            "epoch 1, loss 636.54052734375\n",
            "epoch 1, loss 671.9678955078125\n",
            "epoch 1, loss 651.2428588867188\n",
            "epoch 1, loss 741.32421875\n",
            "epoch 1, loss 656.8123168945312\n",
            "epoch 1, loss 686.2871704101562\n",
            "epoch 1, loss 729.1266479492188\n",
            "epoch 1, loss 686.3203125\n",
            "epoch 1, loss 717.58349609375\n",
            "epoch 1, loss 698.6944580078125\n",
            "epoch 1, loss 645.1437377929688\n",
            "epoch 1, loss 665.100341796875\n",
            "epoch 1, loss 704.3534545898438\n",
            "epoch 1, loss 670.6591186523438\n",
            "epoch 1, loss 671.2866821289062\n",
            "epoch 1, loss 714.0597534179688\n",
            "epoch 1, loss 654.7562255859375\n",
            "epoch 1, loss 710.695556640625\n",
            "epoch 1, loss 639.58056640625\n",
            "epoch 1, loss 646.7835083007812\n",
            "epoch 1, loss 635.3971557617188\n",
            "epoch 1, loss 674.4368286132812\n",
            "epoch 1, loss 605.80029296875\n",
            "epoch 1, loss 681.9049072265625\n",
            "epoch 1, loss 671.73974609375\n",
            "epoch 1, loss 722.3834228515625\n",
            "epoch 1, loss 667.6651611328125\n",
            "epoch 1, loss 648.5426635742188\n",
            "epoch 1, loss 661.7642822265625\n",
            "epoch 1, loss 628.8413696289062\n",
            "epoch 1, loss 680.1199951171875\n",
            "epoch 1, loss 742.954345703125\n",
            "epoch 1, loss 699.22412109375\n",
            "epoch 1, loss 665.4877319335938\n",
            "epoch 1, loss 641.1947631835938\n",
            "epoch 1, loss 712.6270141601562\n",
            "epoch 1, loss 2711.482177734375\n",
            "epoch 1, loss 2725.847412109375\n",
            "epoch 1, loss 668.974609375\n",
            "epoch 1, loss 677.2097778320312\n",
            "epoch 1, loss 650.1961059570312\n",
            "epoch 1, loss 655.1701049804688\n",
            "epoch 1, loss 616.4840087890625\n",
            "epoch 2, loss 630.430419921875\n",
            "epoch 2, loss 669.9339599609375\n",
            "epoch 2, loss 626.6567993164062\n",
            "epoch 2, loss 648.4613037109375\n",
            "epoch 2, loss 646.726318359375\n",
            "epoch 2, loss 635.934326171875\n",
            "epoch 2, loss 646.294677734375\n",
            "epoch 2, loss 630.4631958007812\n",
            "epoch 2, loss 2734.362548828125\n",
            "epoch 2, loss 649.1246337890625\n",
            "epoch 2, loss 691.5599975585938\n",
            "epoch 2, loss 680.7423706054688\n",
            "epoch 2, loss 652.4644775390625\n",
            "epoch 2, loss 647.27734375\n",
            "epoch 2, loss 714.1358032226562\n",
            "epoch 2, loss 694.5233764648438\n",
            "epoch 2, loss 747.31494140625\n",
            "epoch 2, loss 691.2449340820312\n",
            "epoch 2, loss 654.5445556640625\n",
            "epoch 2, loss 624.1057739257812\n",
            "epoch 2, loss 674.1815185546875\n",
            "epoch 2, loss 635.7386474609375\n",
            "epoch 2, loss 668.8012084960938\n",
            "epoch 2, loss 667.4788818359375\n",
            "epoch 2, loss 655.3501586914062\n",
            "epoch 2, loss 610.3319091796875\n",
            "epoch 2, loss 658.656005859375\n",
            "epoch 2, loss 683.9405517578125\n",
            "epoch 2, loss 632.3558349609375\n",
            "epoch 2, loss 703.0107421875\n",
            "epoch 2, loss 701.9136352539062\n",
            "epoch 2, loss 613.9248657226562\n",
            "epoch 2, loss 661.0889892578125\n",
            "epoch 2, loss 627.5597534179688\n",
            "epoch 2, loss 656.9237670898438\n",
            "epoch 2, loss 654.6135864257812\n",
            "epoch 2, loss 616.340576171875\n",
            "epoch 2, loss 648.6087036132812\n",
            "epoch 2, loss 637.445556640625\n",
            "epoch 2, loss 2740.774169921875\n",
            "epoch 2, loss 646.8544311523438\n",
            "epoch 2, loss 676.7567749023438\n",
            "epoch 2, loss 619.6688232421875\n",
            "epoch 2, loss 676.4531860351562\n",
            "epoch 2, loss 717.80078125\n",
            "epoch 2, loss 675.8621826171875\n",
            "epoch 2, loss 656.0191650390625\n",
            "epoch 2, loss 617.5596313476562\n",
            "epoch 2, loss 645.9335327148438\n",
            "epoch 3, loss 609.780029296875\n",
            "epoch 3, loss 632.791748046875\n",
            "epoch 3, loss 632.1770629882812\n",
            "epoch 3, loss 637.7178344726562\n",
            "epoch 3, loss 703.7653198242188\n",
            "epoch 3, loss 2739.75\n",
            "epoch 3, loss 747.3878784179688\n",
            "epoch 3, loss 603.4644775390625\n",
            "epoch 3, loss 701.6754150390625\n",
            "epoch 3, loss 656.88720703125\n",
            "epoch 3, loss 667.39111328125\n",
            "epoch 3, loss 652.9317016601562\n",
            "epoch 3, loss 634.4354858398438\n",
            "epoch 3, loss 660.8232421875\n",
            "epoch 3, loss 583.9909057617188\n",
            "epoch 3, loss 622.1466674804688\n",
            "epoch 3, loss 647.903076171875\n",
            "epoch 3, loss 662.2315673828125\n",
            "epoch 3, loss 627.7515869140625\n",
            "epoch 3, loss 647.43408203125\n",
            "epoch 3, loss 658.6302490234375\n",
            "epoch 3, loss 681.0380859375\n",
            "epoch 3, loss 661.7703247070312\n",
            "epoch 3, loss 615.8971557617188\n",
            "epoch 3, loss 616.4837036132812\n",
            "epoch 3, loss 654.9937744140625\n",
            "epoch 3, loss 646.4710083007812\n",
            "epoch 3, loss 679.827880859375\n",
            "epoch 3, loss 630.5831298828125\n",
            "epoch 3, loss 591.8056030273438\n",
            "epoch 3, loss 694.8108520507812\n",
            "epoch 3, loss 611.866943359375\n",
            "epoch 3, loss 627.494140625\n",
            "epoch 3, loss 621.7583618164062\n",
            "epoch 3, loss 619.3402099609375\n",
            "epoch 3, loss 657.738525390625\n",
            "epoch 3, loss 609.085205078125\n",
            "epoch 3, loss 601.4160766601562\n",
            "epoch 3, loss 631.2589721679688\n",
            "epoch 3, loss 639.718505859375\n",
            "epoch 3, loss 663.721435546875\n",
            "epoch 3, loss 598.24658203125\n",
            "epoch 3, loss 646.7901000976562\n",
            "epoch 3, loss 2777.194091796875\n",
            "epoch 3, loss 614.7789916992188\n",
            "epoch 3, loss 609.6080322265625\n",
            "epoch 3, loss 665.3450927734375\n",
            "epoch 3, loss 685.6129150390625\n",
            "epoch 3, loss 630.0911254882812\n",
            "epoch 4, loss 623.4400024414062\n",
            "epoch 4, loss 663.7041015625\n",
            "epoch 4, loss 713.3403930664062\n",
            "epoch 4, loss 625.0626220703125\n",
            "epoch 4, loss 640.1065673828125\n",
            "epoch 4, loss 625.9779663085938\n",
            "epoch 4, loss 632.446533203125\n",
            "epoch 4, loss 628.6135864257812\n",
            "epoch 4, loss 627.16650390625\n",
            "epoch 4, loss 636.8104248046875\n",
            "epoch 4, loss 631.090087890625\n",
            "epoch 4, loss 595.72509765625\n",
            "epoch 4, loss 622.5792846679688\n",
            "epoch 4, loss 616.9442749023438\n",
            "epoch 4, loss 637.623046875\n",
            "epoch 4, loss 652.82177734375\n",
            "epoch 4, loss 644.4976806640625\n",
            "epoch 4, loss 636.8359375\n",
            "epoch 4, loss 627.4247436523438\n",
            "epoch 4, loss 630.7620239257812\n",
            "epoch 4, loss 666.7112426757812\n",
            "epoch 4, loss 621.579345703125\n",
            "epoch 4, loss 651.5511474609375\n",
            "epoch 4, loss 623.3626708984375\n",
            "epoch 4, loss 653.9892578125\n",
            "epoch 4, loss 625.26708984375\n",
            "epoch 4, loss 2718.45751953125\n",
            "epoch 4, loss 2722.356201171875\n",
            "epoch 4, loss 588.3390502929688\n",
            "epoch 4, loss 608.15380859375\n",
            "epoch 4, loss 581.6937255859375\n",
            "epoch 4, loss 593.968994140625\n",
            "epoch 4, loss 638.2857055664062\n",
            "epoch 4, loss 641.236572265625\n",
            "epoch 4, loss 679.7461547851562\n",
            "epoch 4, loss 622.7489624023438\n",
            "epoch 4, loss 579.7364501953125\n",
            "epoch 4, loss 620.2523193359375\n",
            "epoch 4, loss 679.6775512695312\n",
            "epoch 4, loss 610.7481079101562\n",
            "epoch 4, loss 607.3131713867188\n",
            "epoch 4, loss 603.0391845703125\n",
            "epoch 4, loss 612.7747802734375\n",
            "epoch 4, loss 587.5901489257812\n",
            "epoch 4, loss 664.1084594726562\n",
            "epoch 4, loss 651.031982421875\n",
            "epoch 4, loss 627.12841796875\n",
            "epoch 4, loss 647.9025268554688\n",
            "epoch 4, loss 612.5364379882812\n",
            "epoch 5, loss 673.4694213867188\n",
            "epoch 5, loss 582.7378540039062\n",
            "epoch 5, loss 666.0479125976562\n",
            "epoch 5, loss 572.45458984375\n",
            "epoch 5, loss 607.47412109375\n",
            "epoch 5, loss 664.9428100585938\n",
            "epoch 5, loss 565.3004150390625\n",
            "epoch 5, loss 614.8027954101562\n",
            "epoch 5, loss 669.6983642578125\n",
            "epoch 5, loss 674.25537109375\n",
            "epoch 5, loss 599.0719604492188\n",
            "epoch 5, loss 602.1342163085938\n",
            "epoch 5, loss 600.5671997070312\n",
            "epoch 5, loss 663.4351196289062\n",
            "epoch 5, loss 624.2918701171875\n",
            "epoch 5, loss 632.6260375976562\n",
            "epoch 5, loss 601.137939453125\n",
            "epoch 5, loss 578.5797729492188\n",
            "epoch 5, loss 638.609130859375\n",
            "epoch 5, loss 606.22802734375\n",
            "epoch 5, loss 620.23828125\n",
            "epoch 5, loss 586.1373291015625\n",
            "epoch 5, loss 663.3837280273438\n",
            "epoch 5, loss 545.8787841796875\n",
            "epoch 5, loss 639.0244750976562\n",
            "epoch 5, loss 635.2100219726562\n",
            "epoch 5, loss 622.6585083007812\n",
            "epoch 5, loss 630.5887451171875\n",
            "epoch 5, loss 561.6630249023438\n",
            "epoch 5, loss 665.5187377929688\n",
            "epoch 5, loss 631.6007690429688\n",
            "epoch 5, loss 2680.56298828125\n",
            "epoch 5, loss 2720.4638671875\n",
            "epoch 5, loss 606.62744140625\n",
            "epoch 5, loss 611.630126953125\n",
            "epoch 5, loss 590.2416381835938\n",
            "epoch 5, loss 608.2772827148438\n",
            "epoch 5, loss 648.0363159179688\n",
            "epoch 5, loss 622.589599609375\n",
            "epoch 5, loss 626.7810668945312\n",
            "epoch 5, loss 622.6204833984375\n",
            "epoch 5, loss 577.1361083984375\n",
            "epoch 5, loss 579.370849609375\n",
            "epoch 5, loss 676.5250244140625\n",
            "epoch 5, loss 627.3017578125\n",
            "epoch 5, loss 577.9006958007812\n",
            "epoch 5, loss 647.8056030273438\n",
            "epoch 5, loss 570.0490112304688\n",
            "epoch 5, loss 566.7770385742188\n",
            "epoch 6, loss 602.7183227539062\n",
            "epoch 6, loss 604.1217041015625\n",
            "epoch 6, loss 594.7984008789062\n",
            "epoch 6, loss 641.759521484375\n",
            "epoch 6, loss 603.041259765625\n",
            "epoch 6, loss 615.9397583007812\n",
            "epoch 6, loss 607.7529296875\n",
            "epoch 6, loss 591.8318481445312\n",
            "epoch 6, loss 594.0927734375\n",
            "epoch 6, loss 595.234619140625\n",
            "epoch 6, loss 620.53955078125\n",
            "epoch 6, loss 621.7073974609375\n",
            "epoch 6, loss 637.7718505859375\n",
            "epoch 6, loss 623.4596557617188\n",
            "epoch 6, loss 576.5772705078125\n",
            "epoch 6, loss 534.1452026367188\n",
            "epoch 6, loss 595.271240234375\n",
            "epoch 6, loss 635.4244995117188\n",
            "epoch 6, loss 582.2362060546875\n",
            "epoch 6, loss 598.9053955078125\n",
            "epoch 6, loss 569.4273681640625\n",
            "epoch 6, loss 596.9595947265625\n",
            "epoch 6, loss 589.2171630859375\n",
            "epoch 6, loss 618.18310546875\n",
            "epoch 6, loss 577.2891235351562\n",
            "epoch 6, loss 588.4657592773438\n",
            "epoch 6, loss 570.1103515625\n",
            "epoch 6, loss 602.47314453125\n",
            "epoch 6, loss 643.413330078125\n",
            "epoch 6, loss 617.8721313476562\n",
            "epoch 6, loss 604.2380981445312\n",
            "epoch 6, loss 2715.125\n",
            "epoch 6, loss 588.4898681640625\n",
            "epoch 6, loss 599.3162841796875\n",
            "epoch 6, loss 615.8959350585938\n",
            "epoch 6, loss 2703.909912109375\n",
            "epoch 6, loss 603.3794555664062\n",
            "epoch 6, loss 609.2426147460938\n",
            "epoch 6, loss 606.7581787109375\n",
            "epoch 6, loss 599.0569458007812\n",
            "epoch 6, loss 594.2435302734375\n",
            "epoch 6, loss 572.2184448242188\n",
            "epoch 6, loss 685.0558471679688\n",
            "epoch 6, loss 587.4082641601562\n",
            "epoch 6, loss 566.8934936523438\n",
            "epoch 6, loss 669.2944946289062\n",
            "epoch 6, loss 602.9340209960938\n",
            "epoch 6, loss 649.3397216796875\n",
            "epoch 6, loss 564.1448364257812\n",
            "epoch 7, loss 583.897705078125\n",
            "epoch 7, loss 556.975341796875\n",
            "epoch 7, loss 623.7908325195312\n",
            "epoch 7, loss 582.5735473632812\n",
            "epoch 7, loss 618.1168823242188\n",
            "epoch 7, loss 592.9226684570312\n",
            "epoch 7, loss 622.3837890625\n",
            "epoch 7, loss 609.5752563476562\n",
            "epoch 7, loss 603.8803100585938\n",
            "epoch 7, loss 558.7577514648438\n",
            "epoch 7, loss 596.402587890625\n",
            "epoch 7, loss 607.2081909179688\n",
            "epoch 7, loss 562.8469848632812\n",
            "epoch 7, loss 582.9550170898438\n",
            "epoch 7, loss 612.3097534179688\n",
            "epoch 7, loss 552.3348999023438\n",
            "epoch 7, loss 600.5484619140625\n",
            "epoch 7, loss 583.5411987304688\n",
            "epoch 7, loss 590.5733032226562\n",
            "epoch 7, loss 616.5232543945312\n",
            "epoch 7, loss 601.9371337890625\n",
            "epoch 7, loss 595.4265747070312\n",
            "epoch 7, loss 620.6959228515625\n",
            "epoch 7, loss 605.5846557617188\n",
            "epoch 7, loss 573.8050537109375\n",
            "epoch 7, loss 580.8516845703125\n",
            "epoch 7, loss 601.5724487304688\n",
            "epoch 7, loss 582.4412231445312\n",
            "epoch 7, loss 561.2444458007812\n",
            "epoch 7, loss 610.0242309570312\n",
            "epoch 7, loss 591.13037109375\n",
            "epoch 7, loss 524.1964721679688\n",
            "epoch 7, loss 590.9373168945312\n",
            "epoch 7, loss 2735.624267578125\n",
            "epoch 7, loss 623.5484008789062\n",
            "epoch 7, loss 597.3485107421875\n",
            "epoch 7, loss 628.6351928710938\n",
            "epoch 7, loss 2675.849853515625\n",
            "epoch 7, loss 555.332763671875\n",
            "epoch 7, loss 598.3832397460938\n",
            "epoch 7, loss 598.3360595703125\n",
            "epoch 7, loss 641.7083129882812\n",
            "epoch 7, loss 604.30908203125\n",
            "epoch 7, loss 572.514404296875\n",
            "epoch 7, loss 581.953125\n",
            "epoch 7, loss 559.0131225585938\n",
            "epoch 7, loss 613.9664306640625\n",
            "epoch 7, loss 527.0911254882812\n",
            "epoch 7, loss 603.1203002929688\n",
            "epoch 8, loss 592.7372436523438\n",
            "epoch 8, loss 581.7412109375\n",
            "epoch 8, loss 586.8012084960938\n",
            "epoch 8, loss 615.7596435546875\n",
            "epoch 8, loss 586.1771850585938\n",
            "epoch 8, loss 562.9893188476562\n",
            "epoch 8, loss 601.7295532226562\n",
            "epoch 8, loss 575.865966796875\n",
            "epoch 8, loss 594.2225952148438\n",
            "epoch 8, loss 583.5798950195312\n",
            "epoch 8, loss 625.3235473632812\n",
            "epoch 8, loss 538.5995483398438\n",
            "epoch 8, loss 591.6256103515625\n",
            "epoch 8, loss 636.1753540039062\n",
            "epoch 8, loss 648.6373291015625\n",
            "epoch 8, loss 556.9705810546875\n",
            "epoch 8, loss 589.4481201171875\n",
            "epoch 8, loss 540.2024536132812\n",
            "epoch 8, loss 564.8865966796875\n",
            "epoch 8, loss 625.9530639648438\n",
            "epoch 8, loss 569.3023071289062\n",
            "epoch 8, loss 550.3402709960938\n",
            "epoch 8, loss 582.6451416015625\n",
            "epoch 8, loss 610.8076782226562\n",
            "epoch 8, loss 636.3566284179688\n",
            "epoch 8, loss 586.0909423828125\n",
            "epoch 8, loss 577.8341064453125\n",
            "epoch 8, loss 544.4271850585938\n",
            "epoch 8, loss 519.3392944335938\n",
            "epoch 8, loss 2648.0615234375\n",
            "epoch 8, loss 625.0935668945312\n",
            "epoch 8, loss 578.5242919921875\n",
            "epoch 8, loss 526.1133422851562\n",
            "epoch 8, loss 543.5499877929688\n",
            "epoch 8, loss 578.580078125\n",
            "epoch 8, loss 521.52587890625\n",
            "epoch 8, loss 638.7062377929688\n",
            "epoch 8, loss 582.1408081054688\n",
            "epoch 8, loss 614.6018676757812\n",
            "epoch 8, loss 587.7578125\n",
            "epoch 8, loss 2671.092041015625\n",
            "epoch 8, loss 552.3173828125\n",
            "epoch 8, loss 569.1029663085938\n",
            "epoch 8, loss 577.237548828125\n",
            "epoch 8, loss 585.6417846679688\n",
            "epoch 8, loss 573.013671875\n",
            "epoch 8, loss 550.8030395507812\n",
            "epoch 8, loss 573.4376831054688\n",
            "epoch 8, loss 535.4320068359375\n",
            "epoch 9, loss 577.03271484375\n",
            "epoch 9, loss 585.8582153320312\n",
            "epoch 9, loss 534.699951171875\n",
            "epoch 9, loss 615.0786743164062\n",
            "epoch 9, loss 571.3543090820312\n",
            "epoch 9, loss 536.9813232421875\n",
            "epoch 9, loss 573.231201171875\n",
            "epoch 9, loss 2648.06494140625\n",
            "epoch 9, loss 586.189208984375\n",
            "epoch 9, loss 597.72119140625\n",
            "epoch 9, loss 547.2244873046875\n",
            "epoch 9, loss 2714.089111328125\n",
            "epoch 9, loss 534.5823974609375\n",
            "epoch 9, loss 574.3231201171875\n",
            "epoch 9, loss 561.6371459960938\n",
            "epoch 9, loss 611.1047973632812\n",
            "epoch 9, loss 556.0405883789062\n",
            "epoch 9, loss 566.2033081054688\n",
            "epoch 9, loss 562.7774658203125\n",
            "epoch 9, loss 568.63671875\n",
            "epoch 9, loss 548.0863037109375\n",
            "epoch 9, loss 526.85205078125\n",
            "epoch 9, loss 528.7672119140625\n",
            "epoch 9, loss 538.0963134765625\n",
            "epoch 9, loss 548.8523559570312\n",
            "epoch 9, loss 582.4302978515625\n",
            "epoch 9, loss 576.2589111328125\n",
            "epoch 9, loss 646.0221557617188\n",
            "epoch 9, loss 563.8258666992188\n",
            "epoch 9, loss 585.7630615234375\n",
            "epoch 9, loss 600.260498046875\n",
            "epoch 9, loss 577.6373291015625\n",
            "epoch 9, loss 533.0745849609375\n",
            "epoch 9, loss 549.8894653320312\n",
            "epoch 9, loss 564.0521850585938\n",
            "epoch 9, loss 545.3387451171875\n",
            "epoch 9, loss 586.71435546875\n",
            "epoch 9, loss 585.1032104492188\n",
            "epoch 9, loss 554.927734375\n",
            "epoch 9, loss 559.9124145507812\n",
            "epoch 9, loss 576.1075439453125\n",
            "epoch 9, loss 572.7345581054688\n",
            "epoch 9, loss 566.112548828125\n",
            "epoch 9, loss 563.5553588867188\n",
            "epoch 9, loss 541.9645385742188\n",
            "epoch 9, loss 567.0166625976562\n",
            "epoch 9, loss 601.8653564453125\n",
            "epoch 9, loss 568.1011352539062\n",
            "epoch 9, loss 607.6583251953125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0Qrt31nOqwa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysNMvLQIMd3j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = []\n",
        "captures = {}\n",
        "for i, row in test_ids.iterrows():\n",
        "  capture = row.image_id.split(\"resize\")[1].split(\".\")[0]\n",
        "  if capture not in captures:\n",
        "    captures[capture] = []\n",
        "  captures[capture].append(model(torch.from_numpy(sample_scores[i]).float()).detach().numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lUir-tsOZS7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "with open(\"drive/My Drive/results.csv\", \"w\") as f:\n",
        "  f.write(\"ImageId,PredictionString\\n\")\n",
        "  for c in captures:\n",
        "    s = \" \".join(list(map(str,sum([list(x) + [0.8] for x in captures[c]], []))))\n",
        "    f.write(f\"{c}, {s}\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unWToujvWYlE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b34b47b9-4eea-4414-f75d-2cbca8b7c88d"
      },
      "source": [
        "!cat drive/My\\ Drive/results.csv | wc -l"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}